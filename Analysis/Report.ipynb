{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64be1254",
   "metadata": {},
   "source": [
    "# NER Model Comparison Report\n",
    "\n",
    "## Introduction\n",
    "The purpose of this report is to provide a comparative analysis of the two Named Entity Recognition (NER) models evaluated: **SecureBERT-NER** and **CyNER**. \n",
    "\n",
    "The findings unequivocally demonstrate that the **SecureBERT-NER** model outperforms the alternative across nearly all metrics, with the exception of a negligible difference in processing speed (a margin of only a few percentage points).\n",
    "\n",
    "## Evaluation Process\n",
    "\n",
    "### 1. Direct Testing (Baseline)\n",
    "In the initial phase, a direct baseline comparison was conducted between the two models using the DNRTI dataset. At this stage, SecureBERT-NER already showed a significant lead in performance.\n",
    "- **Reference:** `comparison_v1.ipynb`\n",
    "- **Initial Metrics (F1-Score):**\n",
    "    - **CyNER:** 0.0255\n",
    "    - **SecureBERT-NER:** 0.3057\n",
    "\n",
    "### 2. Label Alignment & Mapping\n",
    "It was identified that the labels were not directly comparable across models as each utilizes its own specific mapping system. To ensure an \"apples-to-apples\" comparison, the label sets were aligned to the DNRTI schema.\n",
    "- **Reference:** `comparison_v2.ipynb`\n",
    "- **Post-Alignment Metrics (F1-Score):**\n",
    "    - **CyNER:** 0.1694\n",
    "    - **SecureBERT-NER:** 0.6606\n",
    "\n",
    "### 3. Data Disparity & Refinement\n",
    "Following the alignment, the performance metrics shifted significantly in favor of SecureBERT-NER. To ensure a fair assessment, further investigations were conducted to see if a more refined label mapping could improve CyNER's performance.\n",
    "- **Reference:** `label_mapping_analysis.ipynb`, `analyze_labels.py`, and `comparison_v3.ipynb`\n",
    "- **Refined Metrics (F1-Score):**\n",
    "    - **CyNER:** 0.3415\n",
    "    - **SecureBERT-NER:** 0.7659\n",
    "\n",
    "![Performance Comparison](Images/performance_comparison_v3.png)\n",
    "*Figure 1: Final Performance Metrics Comparison (from comparison_v3.ipynb).*\n",
    "\n",
    "### 4. Heatmap Analysis\n",
    "A mapping analysis was performed using heatmap visualizations to correlate predicted labels with ground truth. This confirmed—with statistical clarity—that SecureBERT-NER remains the superior choice for this task.\n",
    "\n",
    "#### CyNER Heatmap\n",
    "![CyNER Heatmap](Images/heatmap_cyner.png)\n",
    "*Figure 2: Correlation Heatmap for CyNER Predicted vs DNRTI True Labels.*\n",
    "\n",
    "#### SecureBERT Heatmap\n",
    "![SecureBERT Heatmap](Images/heatmap_securebert.png)\n",
    "*Figure 3: Correlation Heatmap for SecureBERT Predicted vs DNRTI True Labels.*\n",
    "\n",
    "## Key Findings & Terminology\n",
    "*   **Unequivocally Superior:** SecureBERT-NER is the clear winner in accuracy, precision, and recall.\n",
    "*   **Negligible Latency Difference:** While CyNER is slightly faster (approx. 0.0446s vs 0.0452s per doc), the difference is statistically insignificant for most production use cases.\n",
    "*   **Label Mapping:** Crucial for aligning different classification categories to a standard baseline.\n",
    "*   **Baseline:** The initial unoptimized test used to establish the starting performance levels.\n",
    "\n",
    "## Conclusion\n",
    "Based on the extensive evaluation process, **SecureBERT-NER** is recommended for implementation due to its significantly higher F1-score and more reliable entity recognition capabilities.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
